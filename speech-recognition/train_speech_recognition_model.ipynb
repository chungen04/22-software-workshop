{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Test for GPU"
      ],
      "metadata": {
        "id": "N5hMxX4YcoLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "cbE5DCX2cnpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Training Data"
      ],
      "metadata": {
        "id": "GzJYriKkcupW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file(filename, url):\n",
        "    \"\"\"\n",
        "    get file with wget from url if the file does not exist\n",
        "    \"\"\"\n",
        "    COMMAND = f\"\"\"if [ -f {filename} ];\n",
        "    then\n",
        "        echo '{filename} already exists.';\n",
        "    else\n",
        "        wget '{url}';\n",
        "    fi\"\"\"\n",
        "    ! eval \"{COMMAND}\"\n",
        "\n",
        "url_prefix = \"https://github.com/ntueecamp/22-software-workshop/releases/download/dataset/\"\n",
        "\n",
        "file_names = [\n",
        "    \"training_spectrogram.npz.aa\",\n",
        "    \"training_spectrogram.npz.ab\",\n",
        "    \"validation_spectrogram.npz\",\n",
        "    \"test_spectrogram.npz\"\n",
        "]\n",
        "\n",
        "for f in file_names:\n",
        "    get_file(f, url_prefix+f)\n",
        "\n",
        "! if [ -f training_spectrogram.npz ]; \\\n",
        "then \\\n",
        "    echo \"training_spectrogram.npz already exists.\"; \\\n",
        "else \\\n",
        "    cat training_spectrogram.npz.a* > training_spectrogram.npz; \\\n",
        "    echo \"done merging files\"; \\\n",
        "fi"
      ],
      "metadata": {
        "id": "cOq0lvR5cygv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZuigSoBOgNw"
      },
      "source": [
        "# Training\n",
        "\n",
        "This treats the spectrograms of the words like images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa6niXGfOgNx"
      },
      "outputs": [],
      "source": [
        "# Import all the things we will need\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E64TJ9oAcjLz"
      },
      "outputs": [],
      "source": [
        "# clear out any old logs\n",
        "!rm -rf ./logs/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3rOOpdlcjL0"
      },
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EENm2XtOgN6"
      },
      "outputs": [],
      "source": [
        "# List of the words in categorical order\n",
        "command_words = [\n",
        "    'forward',\n",
        "    'backward',\n",
        "    'left',\n",
        "    'right',\n",
        "    '_invalid',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERCnsdwMOgN9"
      },
      "outputs": [],
      "source": [
        "# Load up the sprectrograms and labels\n",
        "training_spectrogram = np.load('training_spectrogram.npz')\n",
        "validation_spectrogram = np.load('validation_spectrogram.npz')\n",
        "test_spectrogram = np.load('test_spectrogram.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IquLGwB6pSwR"
      },
      "outputs": [],
      "source": [
        "# plot a distribution of the words\n",
        "plt.hist(training_spectrogram['Y'], bins=range(0,len(command_words)+1), align='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXHaeU4uqLnM"
      },
      "outputs": [],
      "source": [
        "unique, counts = np.unique(training_spectrogram['Y'], return_counts=True)\n",
        "print(unique, counts)\n",
        "dict(zip([command_words[i] for i in unique], counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L_A47mDOgN_"
      },
      "outputs": [],
      "source": [
        "# extract the data from the files\n",
        "X_train = training_spectrogram['X']\n",
        "X_validate = validation_spectrogram['X']\n",
        "Y_train = tf.one_hot(training_spectrogram['Y'], len(command_words))\n",
        "Y_validate = tf.one_hot(validation_spectrogram['Y'], len(command_words))\n",
        "\n",
        "# get the width and height of the spectrogram \"image\"\n",
        "IMG_WIDTH=X_train[0].shape[0]\n",
        "IMG_HEIGHT=X_train[0].shape[1]\n",
        "\n",
        "train_size = len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-60v8-m3OgOF"
      },
      "outputs": [],
      "source": [
        "# create the datasets for training\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = Dataset.from_tensor_slices(\n",
        "    (X_train, Y_train)\n",
        ").repeat(\n",
        "    count=-1\n",
        ").shuffle(\n",
        "    train_size\n",
        ").batch(\n",
        "    batch_size\n",
        ")\n",
        "\n",
        "validation_dataset = Dataset.from_tensor_slices((X_validate, Y_validate)).batch(X_validate.shape[0]//10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train\n",
        "del X_validate\n",
        "del Y_train\n",
        "del Y_validate\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "vSXiSaQriwDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbnijb64OgOM"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(4, 3, \n",
        "           padding='same',\n",
        "           activation='relu',\n",
        "           kernel_regularizer=regularizers.l2(0.001),\n",
        "           name='conv_layer1',\n",
        "           input_shape=(IMG_WIDTH, IMG_HEIGHT, 1)),\n",
        "    MaxPooling2D(name='max_pooling1', pool_size=(2,2)),\n",
        "    Conv2D(4, 3, \n",
        "           padding='same',\n",
        "           activation='relu',\n",
        "           kernel_regularizer=regularizers.l2(0.001),\n",
        "           name='conv_layer2'),\n",
        "    MaxPooling2D(name='max_pooling3', pool_size=(2,2)),\n",
        "    Flatten(),\n",
        "    Dropout(0.1),\n",
        "    Dense(\n",
        "        80,\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l2(0.001),\n",
        "        name='hidden_layer1'\n",
        "    ),\n",
        "    Dropout(0.1),\n",
        "    Dense(\n",
        "        len(command_words), \n",
        "        activation='softmax',\n",
        "        kernel_regularizer=regularizers.l2(0.001),\n",
        "        name='output'\n",
        "    )\n",
        "])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnEC2AOdOgOO"
      },
      "outputs": [],
      "source": [
        "epochs=3\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUzYHZJ2OgOR"
      },
      "source": [
        "# Logging to tensorboard\n",
        "We log the training stats along with the confusion matrix of the test data - should we be using the validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me2W-gteOgOT"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s--XYYIOgOW"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=train_size // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_dataset,\n",
        "    validation_steps=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1u5dvHuYpsg"
      },
      "outputs": [],
      "source": [
        "model.save(\"trained.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Vok4uqFv6_"
      },
      "source": [
        "# Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_spectrogram['X']\n",
        "Y_test = tf.one_hot(test_spectrogram['Y'], len(command_words))\n",
        "\n",
        "test_dataset = Dataset.from_tensor_slices((X_test, Y_test)).batch(len(X_test))"
      ],
      "metadata": {
        "id": "z09i2pI1mCc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21m-pPzgFmhD"
      },
      "outputs": [],
      "source": [
        "model2 =  keras.models.load_model(\"trained.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74brwGyVGZ7B"
      },
      "outputs": [],
      "source": [
        "results = model2.evaluate(X_test, tf.cast(Y_test, tf.float32), batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRkkvWOKM6hv"
      },
      "outputs": [],
      "source": [
        "predictions = model2.predict(X_test, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8Lh4I71cjL7"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    \"\"\"\n",
        "  Returns a matplotlib figure containing the plotted confusion matrix.\n",
        "\n",
        "  Args:\n",
        "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
        "    class_names (array, shape = [n]): String names of the integer classes\n",
        "  \"\"\"\n",
        "    cm = cm.numpy()\n",
        "    # Normalize the confusion matrix.[c]rea\n",
        "    cm = np.around(cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "\n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    # Use white text if squares are dark; otherwise black.\n",
        "    threshold = cm.max() / 2.0\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")\n",
        "    plt.show()\n",
        "#     return figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "PhdAe7WLcjL7"
      },
      "outputs": [],
      "source": [
        "cm = tf.math.confusion_matrix(\n",
        "    labels=tf.argmax(Y_test, 1), predictions=tf.argmax(predictions, 1)\n",
        ")\n",
        "\n",
        "plot_confusion_matrix(cm, command_words)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train_speech_recognition_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}